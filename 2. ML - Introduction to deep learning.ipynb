{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going deep: Deep neural networks\n",
    "\n",
    "So far, we've learned that if we want to classify more than two fruits, we'll need to go beyond using a single neuron and use *multiple* neurons to get multiple outputs. We can think of stacking these multiple neurons together in a single neural layer.\n",
    "\n",
    "Even so, we found that using a single neural layer was not enough to fully distinguish between bananas, grapes, **and** apples. To do this properly, we'll need to add more complexity to our model. We need not just a neural network, but a *deep neural network*. \n",
    "\n",
    "There is one step remaining to build a deep neural network. We have been saying that a neural network takes in data and then spits out `0` or `1` predictions that together declare what kind of fruit the picture is. However, what if we instead put the output of one neural network layer into another neural network layer?\n",
    "\n",
    "This gets pictured like this below:\n",
    "\n",
    "<img src=\"data/deep-neural-net.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "On the left we have 3 data points in blue. Those 3 data points each get fed into 4 neurons in purple. Each of those 4 neurons produces a single output, but those output are each fed into three neurons (the second layer of purple). Each of those 3 neurons spits out a single value, and those values are fed as inputs into the last layer of 6 neurons. The 6 values that those final neurons produce are the output of the neural network. This is a deep neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why would a deep neural network be better?\n",
    "\n",
    "This is a little perplexing when you first see it. We used neurons to train the model before: why would sticking the output from neurons into other neurons help us fit the data better? The answer can be understood by drawing pictures. Geometrically, the matrix multiplication inside of a layer of neurons is streching and rotating the axis that we can vary:\n",
    "\n",
    "![](data/17-raw.png)\n",
    "\n",
    "A nonlinear transformation, such as the sigmoid function, then adds a bump to this linearly-transformed data:\n",
    "\n",
    "![](data/17-linear1.png)\n",
    "\n",
    "Resulting in a bit more of the data accounted for:\n",
    "\n",
    "![](data/17-nonlinear1.png)\n",
    "\n",
    "Now let's repeat this process. When we send the data through another layer of neurons, we get another rotation and another bump:\n",
    "\n",
    "![](data/17-linear2.png)\n",
    "\n",
    "![](data/17-nonlinear2.png)\n",
    "\n",
    "\n",
    "Visually, we see that if we keep doing this process we can make the axis line up with any data. What this means is that **if we have enough layers, then our neural network can approximate any model**. \n",
    "\n",
    "The trade-off is that with more layers we have more parameters, so it may be harder (i.e. computationally intensive) to train the neural network. But we have the guarantee that the model has enough freedom such that there are parameters that will give the correct output. \n",
    "\n",
    "Because this model is so flexible, the problem is reduced to that of learning: do the same gradient descent method on this much larger model (but more efficiently!) and we can make it classify our data correctly. This is the power of deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple cartoon plots to demonstrate pseudo-rotation and \"bump\"\n",
    "using Plots\n",
    "xs = range(0, stop=10, length=200)\n",
    "ys = randn.() .+ xs .+ 0.5.*xs.^1.5 .+ 3.0.*sin.(clamp.(xs .- 5, 0, Inf)) .+ 2.6.*clamp.(.-abs.(xs .- 2), -2, 0)\n",
    "scatter(xs, ys, label=\"\", ticks = false)\n",
    "savefig(\"data/17-raw.png\")\n",
    "\n",
    "fit1 = [xs ones(size(xs))] \\ ys\n",
    "linear1(x) = fit1[1]*x + fit1[2]\n",
    "plot!(linear1, label=\"\")\n",
    "\n",
    "scatter(xs, ys .- linear1.(xs), ticks = false, label = \"\")\n",
    "savefig(\"data/17-linear1.png\")\n",
    "\n",
    "nonlinearity(x) = clamp.(2.0.*x .- 2, -2, 2)\n",
    "plot!(nonlinearity, label = \"\")\n",
    "\n",
    "ys2 = ys .- linear1.(xs) .- nonlinearity.(xs)\n",
    "scatter(xs, ys2, ticks = false, label = \"\")\n",
    "savefig(\"data/17-nonlinear1.png\")\n",
    "\n",
    "fit2 = [xs ones(size(xs))] \\ ys2\n",
    "linear2(x) = fit2[1]*x + fit2[2]\n",
    "plot!(linear2)\n",
    "\n",
    "ys3 = ys2 .- linear2.(xs)\n",
    "scatter(xs .- 2, .-ys3, ticks = false, label = \"\")\n",
    "savefig(\"data/17-linear2.png\")\n",
    "\n",
    "plot!(nonlinearity, label = \"\")\n",
    "\n",
    "ys4 = .-ys3 .- nonlinearity.(xs .- 2)\n",
    "scatter(xs, ys4, ticks = false, label = \"\")\n",
    "savefig(\"data/17-nonlinear2.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.0",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
